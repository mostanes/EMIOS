\section{Introduction}
\subsection{What do we refer to as middle-level interfaces?}
In the past years, computers have become widespread and are supposed to become easier and more efficient to use. As the more advanced technology of today builds on top of yesterday's technology, what once was considered high-level in operating systems (the C language, databases, web servers and middleware) are now core technologies on which more complex software is built, thus creating a middle level in computer architecture. To enable building high-level software efficiently, good interfaces at the middle level of the software stack are now necessary. Finally, a few POSIX APIs were designed for computer architectures decades old and are so far abstracted from efficient hardware paradigms, that the design decisions are obsolete for the high efficiency designs required to attain peak performance at current hardware challenges.
\subsection{On computers being smart}
This subsection is a prose describing the vision behind the original version of this document and is outside the guidelines that make the scope of this document. It should not be kept when the guidelines are transferred into a standard.\\
\hrule\quad

One of the most marketed terms at the time this document was written was "Artificial Intelligence". To understand why this sells and why it is important for this document, one needs to look at what humans expect from computing and what we know about intelligence.\\

In the first place, let us look at the history of computing. Up to the 19th century, primitive computers were designed to only do the basic arithmetic operations, and even those were often cumbersome to do. Only after Lovelace and Turing did computer become general-purpose. The latter is in fact a more accurate reference -- around WW2 stored-program computers started being designed and built (Zuse, Universal Turing Machine, EDVAC). Perhaps not surprisingly, it was at this time that computers began to be seen as a powerful extension of the human mind (Vannevar Bush).\\

The transition to the stored-program architecture and human-centric computers (instead of computer-centric humans) mirrors the development of tools in prehistory. If the primitive people of old used to crack nuts open with huge stones, like some (other) primates do today, someone once strapped a wooden stick to a smaller stone -- which made it into a hammer, a far more convenient tool. It is precisely this convenience that humans also seek from AI (which can be seen as some sort of pinnacle of stored-program computers). It is therefore clear why "AI" is such an important area to computing.\\

How can Artificial Intelligence be programmed, or, in a more daring proposal, bootstrapped into computers? First attempts were through the use of decision trees, but these were quickly overwhelmed by the non-rigid rules of humans and messy inputs. This was followed by statistical approaches, which, due to their lack of inference abilities, could not even come close to "intelligence". More recently, machine learning, referring to (artificial) neural networks and similar techniques, became the favorite method, as it demonstrated significant progress and useful products. Given that neural networks could be argued to sit at the intersection of symbolic regression and statistical methods, one may not be so surprised at their success (the Universal Approximation Theorem may be invoked, but it only tells it is theoretically possible rather than practically doable). However, the same argument also shows the weakness of machine learning: higher-order reasoning is impractical.\\

The reason for this failure is simple if one considers the DIKW (data, information, knowledge, wisdom) pyramid: machine learning models lack knowledge. In fact, it can be seen that knowledge is used in training ML models, which themselves are information. This information acts upon the input, which is data, yield the model output, which is also data. What distinguishes the upper layers (information, knowledge) from the data layer is nothing more than the presence of problem-domain context.\\

The growing complexity of the software required to make computing "intelligent" has pushed management of computers far outside the abilities of the mere human mind. The sheer number of configuration options that need to be set in programs for even a "basic" action (to a modern computer user), such as a lookup on a web search engine, to complete is beyond the ability of a single person to understand (as can be seen from existing software distributions). As such, it has become apparent that autonomic computing is a requirement to pushing software beyond the current level.\\

If operating systems have traditionally dealt with data, in order to enable autonomic computing, it is now time for them to ascend to information processing -- and this requires context. The purpose of this document is to define the framework for building this context into operating systems.
\hrule